{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "> Sources:\n",
    "* http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "* http://jalammar.github.io/illustrated-transformer/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.transpose(x, 0, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "copus_a = [\"one is one\", \"two is two\", \"three is three\", \"four is four\", \"five is five\",\n",
    "           \"six is six\", \"seven is seven\", \"eight is eight\", \"nine is nine\"]\n",
    "copus_b = [\"1 la 1\", \"2 la 2\", \"3 la 3\", \"4 la 4\", \"5 la 5\",\n",
    "           \"6 la 6\", \"7 la 7\", \"8 la 8\", \"9 la 9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_a = {\"is\":   [1.0,0,0,0,0,0,0,0,0,0],\n",
    "           \"one\":  [0,1.0,0,0,0,0,0,0,0,0],\n",
    "           \"two\":  [0,0,1.0,0,0,0,0,0,0,0],\n",
    "           \"three\":[0,0,0,1.0,0,0,0,0,0,0],\n",
    "           \"four\": [0,0,0,0,1.0,0,0,0,0,0],\n",
    "           \"five\": [0,0,0,0,0,1.0,0,0,0,0],\n",
    "           \"six\":  [0,0,0,0,0,0,1.0,0,0,0],\n",
    "           \"seven\":[0,0,0,0,0,0,0,1.0,0,0],\n",
    "           \"eight\":[0,0,0,0,0,0,0,0,1.0,0],\n",
    "           \"nine\": [0,0,0,0,0,0,0,0,0,1.0]}\n",
    "\n",
    "embed_b = {\"9\": [1.0,0,0,0,0,0,0,0,0,0],\n",
    "           \"8\": [0,1.0,0,0,0,0,0,0,0,0],\n",
    "           \"7\": [0,0,1.0,0,0,0,0,0,0,0],\n",
    "           \"6\": [0,0,0,1.0,0,0,0,0,0,0],\n",
    "           \"5\": [0,0,0,0,1.0,0,0,0,0,0],\n",
    "           \"4\": [0,0,0,0,0,1.0,0,0,0,0],\n",
    "           \"3\": [0,0,0,0,0,0,1.0,0,0,0],\n",
    "           \"2\": [0,0,0,0,0,0,0,1.0,0,0],\n",
    "           \"1\": [0,0,0,0,0,0,0,0,1.0,0],\n",
    "           \"la\":[0,0,0,0,0,0,0,0,0,1.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_embed(sentence, embed_dict):\n",
    "    res = []\n",
    "    for word in sentence.split():\n",
    "        res.append(embed_dict[word])\n",
    "    return res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10]), torch.Size([3, 10]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = sentence_embed(\"one is one\", embed_a) \n",
    "out = sentence_embed(\"1 la 1\", embed_b)\n",
    "inp = torch.tensor(inp, dtype=torch.float32)\n",
    "out = torch.tensor(out, dtype=torch.float32)\n",
    "inp.shape, out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_attention(inp, dk):\n",
    "    # Initiate weight matrix for Query, Key and Value\n",
    "    wq, wk, wv = [torch.rand(inp.size(-1), dk, requires_grad=True) for i in range(3)]\n",
    "    q,k,v = inp @ wq, inp @ wk, inp @ wv\n",
    "    logit = (q @ k.transpose(0, -1)) / math.sqrt(dk)\n",
    "    weigt = torch.softmax(logit, dim=-1)\n",
    "    res = weigt @ v\n",
    "    return weigt, res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3219, 0.3561, 0.3219],\n",
       "         [0.2966, 0.4068, 0.2966],\n",
       "         [0.3219, 0.3561, 0.3219]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.3128, 0.4050, 0.9274, 0.2796, 0.2709, 0.6891, 0.6944, 0.5752],\n",
       "         [0.3533, 0.4207, 0.9196, 0.2763, 0.2897, 0.6940, 0.6767, 0.5981],\n",
       "         [0.3128, 0.4050, 0.9274, 0.2796, 0.2709, 0.6891, 0.6944, 0.5752]],\n",
       "        grad_fn=<MmBackward>))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_attention(inp, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, inp, dk):\n",
    "        super().__init__()\n",
    "        self.inp, self.dk = inp, dk\n",
    "        # Initiate weights for Query, Key and Value\n",
    "        self.wq, self.wk, self.wv = [torch.rand(self.inp.size(-1), dk) \n",
    "                                     for i in range(3)]\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return self._dot_attention(inp, self.dk)\n",
    "        \n",
    "    def _dot_attention(self,inp, dk):\n",
    "        # Initiate weight matrix for Query, Key and Value\n",
    "        q,k,v = inp @ self.wq, inp @ self.wk, inp @ self.wv\n",
    "        logit = (q @ k.transpose(0, -1)) / math.sqrt(dk)\n",
    "        weigt = torch.softmax(logit, dim=-1)\n",
    "        res = weigt @ v\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5275, 0.7369, 0.8443, 0.7131, 0.4083, 0.2168, 0.2937, 0.7003],\n",
       "        [0.5221, 0.7481, 0.8478, 0.7108, 0.4193, 0.1963, 0.2772, 0.7107],\n",
       "        [0.5275, 0.7369, 0.8443, 0.7131, 0.4083, 0.2168, 0.2937, 0.7003]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "dk = 8\n",
    "satten = SelfAttention(inp, dk)\n",
    "satten(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, inp, dk, nh):\n",
    "        \"\"\"\n",
    "        inp: input\n",
    "        dk: key dimension\n",
    "        nh: number of heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.inp, self.dk, self.nh = inp, dk, nh\n",
    "        self.layers = [SelfAttention(inp, dk) for i in range(nh)]\n",
    "        self.out = torch.rand(dk*nh, dk, requires_grad=True)        \n",
    "        \n",
    "    def forward(self, inp):\n",
    "        res = []\n",
    "        for l in self.layers:\n",
    "            res.append(l(inp))\n",
    "        ccat = torch.cat(res, 1)\n",
    "        res = ccat @ self.out\n",
    "        return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk = 8\n",
    "nh = 6\n",
    "mul_head = MultiHeadAttention(inp, dk, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13.4013, 15.1956, 16.2374, 16.2639, 11.9684, 11.9073, 13.4298, 14.3592],\n",
       "        [13.3845, 15.1707, 16.2030, 16.2503, 11.9526, 11.9179, 13.4255, 14.3424],\n",
       "        [13.4013, 15.1956, 16.2374, 16.2639, 11.9684, 11.9073, 13.4298, 14.3592]],\n",
       "       grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul_head(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
